{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XdDiy6Fw9ZQ"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Reshape, BatchNormalization, Input, Embedding, Concatenate, Flatten, Conv2DTranspose, Activation\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = [\"The cat is on the mat.\", \"The dog is chasing the cat.\"]\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_data)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert text to sequences\n",
        "sequences = tokenizer.texts_to_sequences(text_data)\n",
        "\n",
        "# Padding\n",
        "max_sequence_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# One-hot encoding\n",
        "one_hot_sequences = np.eye(vocab_size)[padded_sequences]"
      ],
      "metadata": {
        "id": "CAPTsP6exCuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image_directory = \"/path/to/image/directory\"\n",
        "target_image_size = (64, 64)\n",
        "\n",
        "# ImageDataGenerator for loading and preprocessing images\n",
        "image_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\n",
        "# Create a flow_from_directory generator\n",
        "image_generator = image_datagen.flow_from_directory(\n",
        "    image_directory,\n",
        "    target_size=target_image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=None  # We don't need class labels for text-to-image GAN\n",
        ")"
      ],
      "metadata": {
        "id": "Wf5VqjZXxGbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_batch(text_sequences, image_generator, batch_size):\n",
        "    while True:\n",
        "        # Generate a batch of text samples\n",
        "        text_indices = np.random.randint(0, len(text_sequences), size=batch_size)\n",
        "        batch_text = text_sequences[text_indices]\n",
        "\n",
        "        # Generate a batch of image samples\n",
        "        batch_images = next(image_generator)\n",
        "\n",
        "        yield batch_text, batch_images"
      ],
      "metadata": {
        "id": "bFM2AC_KxJDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "# Define the image size\n",
        "image_size = 64\n",
        "\n",
        "# Define the generator model\n",
        "def build_generator():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(256 * 8 * 8, input_dim=embedding_dim))\n",
        "    model.add(Reshape((8, 8, 256)))\n",
        "    \n",
        "    model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Conv2DTranspose(3, kernel_size=4, strides=2, padding='same'))\n",
        "    model.add(Activation('tanh'))\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Define the discriminator model\n",
        "def build_discriminator():\n",
        "    image_input = Input(shape=(image_size, image_size, 3))\n",
        "    x = Conv2D(64, kernel_size=4, strides=2, padding='same')(image_input)\n",
        "    x = LeakyReLU()(x)\n",
        "    \n",
        "    x = Conv2D(128, kernel_size=4, strides=2, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    \n",
        "    x = Conv2D(256, kernel_size=4, strides=2, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    \n",
        "    x = Flatten()(x)\n",
        "    validity = Dense(1, activation='sigmoid')(x)\n",
        "    \n",
        "    discriminator = Model(image_input, validity)\n",
        "    return discriminator\n",
        "\n",
        "# Build the text-to-image GAN\n",
        "def build_text2image_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    \n",
        "    text_input = Input(shape=(embedding_dim,))\n",
        "    generated_image = generator(text_input)\n",
        "    \n",
        "    validity = discriminator(generated_image)\n",
        "    \n",
        "    gan = Model(text_input, validity)\n",
        "    return gan\n",
        "\n",
        "# Instantiate the models\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "text2image_gan = build_text2image_gan(generator, discriminator)\n",
        "\n",
        "# Compile the models\n",
        "optimizer = Adam(0.0002, 0.5)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "text2image_gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n"
      ],
      "metadata": {
        "id": "Gu-T5uHExNMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W5MOQOz2yeEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of epochs and batch size\n",
        "epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "# Set the number of steps per epoch\n",
        "steps_per_epoch = len(text_sequences) // batch_size\n",
        "\n",
        "# Set the path to save generated images\n",
        "output_path = \"/path/to/save/generated/images/\"\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    print(\"Epoch {}/{}\".format(epoch + 1, epochs))\n",
        "\n",
        "    # Initialize the loss and accuracy for each epoch\n",
        "    disc_loss_epoch = 0\n",
        "    disc_acc_epoch = 0\n",
        "    gan_loss_epoch = 0\n",
        "\n",
        "    # Iterate over the batches of data\n",
        "    for step in range(steps_per_epoch):\n",
        "        # Get a batch of text and image samples\n",
        "        batch_text, batch_images = next(get_data_batch(text_sequences, image_generator, batch_size))\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "        \n",
        "        # Generate fake images from text\n",
        "        fake_images = generator.predict(batch_text)\n",
        "\n",
        "        # Train the discriminator\n",
        "        real_labels = np.ones((batch_size, 1))\n",
        "        fake_labels = np.zeros((batch_size, 1))\n",
        "        \n",
        "        d_loss_real = discriminator.train_on_batch(batch_images, real_labels)\n",
        "        d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n",
        "        disc_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "        disc_loss_epoch += disc_loss[0]\n",
        "        disc_acc_epoch += disc_loss[1]\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "\n",
        "        # Generate random noise vectors for the generator\n",
        "        noise = np.random.normal(0, 1, (batch_size, embedding_dim))\n",
        "\n",
        "        # Train the generator (text-to-image GAN)\n",
        "        gan_loss = text2image_gan.train_on_batch(batch_text, real_labels)\n",
        "        gan_loss_epoch += gan_loss\n",
        "\n",
        "    # Calculate and display the average losses and accuracies for the epoch\n",
        "    avg_disc_loss = disc_loss_epoch / steps_per_epoch\n",
        "    avg_disc_acc = disc_acc_epoch / steps_per_epoch\n",
        "    avg_gan_loss = gan_loss_epoch / steps_per_epoch\n",
        "\n",
        "    print(\"Discriminator Loss: {:.4f}, Accuracy: {:.2f}%\".format(avg_disc_loss, avg_disc_acc * 100))\n",
        "    print(\"Generator Loss: {:.4f}\".format(avg_gan_loss))\n",
        "\n",
        "    # Save generated images for visualization\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        random_text = np.random.normal(0, 1, (batch_size, embedding_dim))\n",
        "        generated_images = generator.predict(random_text)\n",
        "        save_generated_images(generated_images, output_path, epoch + 1)\n"
      ],
      "metadata": {
        "id": "sG4qM5iSxcaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the generator model\n",
        "generator.save('path/to/generator_model.h5')\n",
        "\n",
        "# Save the tokenizer\n",
        "np.save('path/to/word_index.npy', tokenizer.word_index)"
      ],
      "metadata": {
        "id": "tSjLyc_Iyfpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# using the model with fastapi"
      ],
      "metadata": {
        "id": "RIa-3bvTytUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install fastapi uvicorn\n",
        "\n",
        "\n",
        "from fastapi import FastAPI\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Create the FastAPI instance\n",
        "app = FastAPI()\n",
        "\n",
        "# Load the text-to-image GAN model\n",
        "generator = tf.keras.models.load_model('path/to/generator_model.h5')\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.word_index = np.load('path/to/word_index.npy', allow_pickle=True).item()\n",
        "max_sequence_length = 10  # Replace with the maximum sequence length used during training\n",
        "\n",
        "# Define the API endpoint for generating images\n",
        "@app.post(\"/generate_image\")\n",
        "async def generate_image(text: str):\n",
        "    # Preprocess the text\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    sequence = pad_sequences(sequence, maxlen=max_sequence_length)\n",
        "\n",
        "    # Generate image\n",
        "    noise = np.random.normal(0, 1, (1, embedding_dim))\n",
        "    generated_image = generator.predict([noise, sequence])\n",
        "\n",
        "    # Convert the generated image to PIL Image format\n",
        "    generated_image = (generated_image[0] * 127.5 + 127.5).astype(np.uint8)\n",
        "    pil_image = Image.fromarray(generated_image)\n",
        "\n",
        "    # Convert the PIL Image to bytes for API response\n",
        "    image_bytes = BytesIO()\n",
        "    pil_image.save(image_bytes, format='JPEG')\n",
        "    image_bytes.seek(0)\n",
        "\n",
        "    return {\"image\": image_bytes}\n",
        "\n",
        "\n",
        "# Run the FastAPI application using Uvicorn server\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(\"app:app\", host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "# uvicorn app:app --reload"
      ],
      "metadata": {
        "id": "chC3lgn5x_5U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}