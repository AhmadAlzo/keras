{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptvIZ7ul5Fys"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example training data\n",
        "input_texts = [...]  # List of input texts\n",
        "target_texts = [...]  # List of corresponding target texts\n",
        "\n",
        "# Tokenize the input and target texts\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(input_texts + target_texts)\n",
        "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
        "target_sequences = tokenizer.texts_to_sequences(target_texts)\n",
        "\n",
        "# Create the vocabulary mapping\n",
        "word_index = tokenizer.word_index\n",
        "num_tokens = len(word_index) + 1  # Add 1 for the padding token\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "max_seq_length = ...  # Maximum sequence length\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_length, padding='post')\n",
        "target_sequences = pad_sequences(target_sequences, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "# Prepare the input and target data\n",
        "encoder_input_data = input_sequences\n",
        "decoder_input_data = target_sequences[:, :-1]  # Remove the last token from target_sequences\n",
        "decoder_target_data = to_categorical(target_sequences[:, 1:], num_tokens)  # Shift target_sequences by 1 and one-hot encode\n"
      ],
      "metadata": {
        "id": "E51ltCk97J8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_encoder_tokens = ...  # Number of unique input tokens\n",
        "num_decoder_tokens = ...  # Number of unique output tokens\n",
        "latent_dim = ...  # Dimensionality of the latent space (encoder/decoder hidden units)\n"
      ],
      "metadata": {
        "id": "BWwRvIir5MO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n"
      ],
      "metadata": {
        "id": "Opl-rBvf5PXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n"
      ],
      "metadata": {
        "id": "LpECWmaQ5YiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ],
      "metadata": {
        "id": "X1AXaszB5b_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs, validation_split=validation_split)\n"
      ],
      "metadata": {
        "id": "S9NRXq_z5jN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('model_file.h5')\n",
        "tokenizer.save('tokenizer_file.h5')"
      ],
      "metadata": {
        "id": "r-RK-Zaj9xwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n"
      ],
      "metadata": {
        "id": "5GHlVkgt5nY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_input_text(input_text, tokenizer, max_seq_length):\n",
        "    # Tokenize the input_text\n",
        "    input_tokens = tokenizer.texts_to_sequences([input_text])[0]\n",
        "\n",
        "    # Pad the sequence to the maximum sequence length\n",
        "    padded_input_tokens = pad_sequences([input_tokens], maxlen=max_seq_length, padding='post')\n",
        "\n",
        "    # Convert to a 2D numpy array with shape (1, max_seq_length)\n",
        "    encoder_input_data = np.array(padded_input_tokens)\n",
        "\n",
        "    return encoder_input_data\n",
        "\n",
        "input_text = \"Hello, how are you?\"\n",
        "\n",
        "# Assuming you have a tokenizer and max_seq_length defined\n",
        "encoder_input_data = encode_input_text(input_text, tokenizer, max_seq_length)\n",
        "\n"
      ],
      "metadata": {
        "id": "sWigA-dc5pIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_response(input_text):\n",
        "    encoder_input_data = encode_input_text(input_text)\n",
        "    states_value = encoder_model.predict(encoder_input_data)\n",
        "\n",
        "    target_seq = ...  # Start with a sequence containing the start token\n",
        "    stop_condition = False\n",
        "    response = \"\"\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token from the output distribution\n",
        "        sampled_token_index = ...  # Select the token with the highest probability\n",
        "        sampled_token = ...  # Convert the token into its corresponding word\n",
        "\n",
        "        if sampled_token != 'end_token':\n",
        "            response += sampled_token + \" \"\n",
        "\n",
        "        # Exit condition: either hitting max length or finding the stop token\n",
        "        if sampled_token == 'end_token' or len(response.split()) > max_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (previous output) and states\n",
        "        target_seq = ...  # Prepare the next token for the decoder input\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "8Qw4hNq39V_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = ...  # User's input\n",
        "response = decode_response(input_text)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "ao3NVM2g5rFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fastapi"
      ],
      "metadata": {
        "id": "fhLED-VF6EB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Define the input and output dimensions\n",
        "num_encoder_tokens = ...\n",
        "num_decoder_tokens = ...\n",
        "latent_dim = ...\n",
        "\n",
        "# Define the encoder model\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Define the decoder model\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the entire model (encoder + decoder)\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Load the trained weights into the model\n",
        "model.load_weights('path_to_weights.h5')\n",
        "\n",
        "async def load_tokenizer():\n",
        "    global tokenizer  # Declare the tokenizer as a global variable\n",
        "    tokenizer = Tokenizer()\n",
        "    # Load the tokenizer from file or any other method\n",
        "    tokenizer.load('tokenizer_file.h5')\n",
        "\n",
        "class ChatRequest(BaseModel):\n",
        "    text: str\n",
        "\n",
        "class ChatResponse(BaseModel):\n",
        "    response: str\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat(request: ChatRequest) -> ChatResponse:\n",
        "    input_text = request.text\n",
        "    response_text = decode_response(input_text)\n",
        "    return ChatResponse(response=response_text)\n",
        "\n",
        "def encode_input_text(input_text, tokenizer, max_seq_length):\n",
        "    # Tokenize the input_text\n",
        "    input_tokens = tokenizer.texts_to_sequences([input_text])[0]\n",
        "\n",
        "    # Pad the sequence to the maximum sequence length\n",
        "    padded_input_tokens = pad_sequences([input_tokens], maxlen=max_seq_length, padding='post')\n",
        "\n",
        "    # Convert to a 2D numpy array with shape (1, max_seq_length)\n",
        "    encoder_input_data = np.array(padded_input_tokens)\n",
        "\n",
        "    return encoder_input_data\n",
        "def decode_response(input_text):\n",
        "    encoder_input_data = encode_input_text(input_text)\n",
        "    states_value = encoder_model.predict(encoder_input_data)\n",
        "\n",
        "    target_seq = ...  # Start with a sequence containing the start token\n",
        "    stop_condition = False\n",
        "    response = \"\"\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token from the output distribution\n",
        "        sampled_token_index = ...  # Select the token with the highest probability\n",
        "        sampled_token = ...  # Convert the token into its corresponding word\n",
        "\n",
        "        if sampled_token != 'end_token':\n",
        "            response += sampled_token + \" \"\n",
        "\n",
        "        # Exit condition: either hitting max length or finding the stop token\n",
        "        if sampled_token == 'end_token' or len(response.split()) > max_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (previous output) and states\n",
        "        target_seq = ...  # Prepare the next token for the decoder input\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return response\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ],
      "metadata": {
        "id": "_P4JaSwi6FXS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}